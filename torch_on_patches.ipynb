{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: validators in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (0.18.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from validators) (1.16.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from validators) (5.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\chiro\\.conda\\envs\\deeplearning\\lib\\site-packages (from matplotlib) (2.8.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install validators matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and download pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# del variables\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms    \n",
    "from torch.optim import lr_scheduler as lrs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# For evaluation and submission\n",
    "from GLC.metrics import top_30_error_rate, top_k_error_rate_from_sets, predict_top_30_set\n",
    "from GLC.submission import generate_submission_file\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# For data loading and visualization\n",
    "from GLC.data_loading.common import load_patch\n",
    "from GLC.plotting import visualize_observation_patch\n",
    "from GLC.data_loading.environmental_raster import PatchExtractor\n",
    "\n",
    "# For time monitoring\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chiro/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the model into a classifier with the 17k+ classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\chiro/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
      "Using cache found in C:\\Users\\chiro/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (stem): Conv2d(6, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (expand): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=96, out_features=4, bias=True)\n",
       "          (expand): Linear(in_features=4, out_features=96, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=144, out_features=6, bias=True)\n",
       "          (expand): Linear(in_features=6, out_features=144, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          (bn): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=144, out_features=6, bias=True)\n",
       "          (expand): Linear(in_features=6, out_features=144, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=240, out_features=10, bias=True)\n",
       "          (expand): Linear(in_features=10, out_features=240, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=240, out_features=10, bias=True)\n",
       "          (expand): Linear(in_features=10, out_features=240, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=480, out_features=20, bias=True)\n",
       "          (expand): Linear(in_features=20, out_features=480, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=672, out_features=28, bias=True)\n",
       "          (expand): Linear(in_features=28, out_features=672, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block1): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block2): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "      (block3): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (block0): MBConvBlock(\n",
       "        (expand): Sequential(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (depsep): Sequential(\n",
       "          (conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (se): SequentialSqueezeAndExcitation(\n",
       "          (squeeze): Linear(in_features=1152, out_features=48, bias=True)\n",
       "          (expand): Linear(in_features=48, out_features=1152, bias=True)\n",
       "          (activation): SiLU(inplace=True)\n",
       "          (sigmoid): Sigmoid()\n",
       "          (mul_a_quantizer): Identity()\n",
       "          (mul_b_quantizer): Identity()\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (residual_quantizer): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (features): Sequential(\n",
       "    (conv): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (activation): SiLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (squeeze): Flatten()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=1280, out_features=4096, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=17037, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_convnets_processing_utils')\n",
    "\n",
    "model.eval().to(device)\n",
    "\n",
    "model.stem = nn.Conv2d(6, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "\n",
    "model.classifier.fc = nn.Sequential(\n",
    "    nn.Linear(in_features=1280, out_features=4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=17037)\n",
    ")\n",
    "\n",
    "# Unfreeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)  #they take 0.01\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "scheduler = lrs.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations loading\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>species_id</th>\n",
       "      <th>subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10561949</td>\n",
       "      <td>45.705116</td>\n",
       "      <td>1.424622</td>\n",
       "      <td>241</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10131188</td>\n",
       "      <td>45.146973</td>\n",
       "      <td>6.416794</td>\n",
       "      <td>101</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   observation_id   latitude  longitude  species_id subset\n",
       "0        10561949  45.705116   1.424622         241  train\n",
       "1        10131188  45.146973   6.416794         101  train"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landcover_code</th>\n",
       "      <th>original_landcover_code</th>\n",
       "      <th>landcover_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Annual Summer Crops</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landcover_code  original_landcover_code      landcover_label\n",
       "0               0                        0         Missing Data\n",
       "1               1                       11  Annual Summer Crops"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landcover_code</th>\n",
       "      <th>suggested_landcover_code</th>\n",
       "      <th>suggested_landcover_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Cultivated Crops</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landcover_code  suggested_landcover_code suggested_landcover_label\n",
       "0               0                         0              Missing Data\n",
       "1               1                        11          Cultivated Crops"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"./geolifeclef-2022-lifeclef-2022-fgvc9\")\n",
    "\n",
    "\n",
    "################\n",
    "# DATA LOADING #\n",
    "################\n",
    "print(\"Observations loading\")\n",
    "\n",
    "# Load train set of observations from France and USA and merge\n",
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
    "\n",
    "# Extract training and validation subsets as np arrays\n",
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "# Separate values to predict\n",
    "# y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "# y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "df_train =  df_obs.loc[obs_id_train].reset_index(drop=False)\n",
    "df_val =  df_obs.loc[obs_id_val].reset_index(drop=False)\n",
    "display(df_train.head(2))\n",
    "\n",
    "# Same with test set of observations\n",
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "# Extract observaions as np array\n",
    "obs_id_test = df_obs_test.index.values\n",
    "df_test =  df_obs_test.loc[obs_id_test].reset_index(drop=False)\n",
    "\n",
    "# Load landcover metadata to use the patches\n",
    "df_landcover_labels = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_original_labels.csv\", sep=\";\")\n",
    "df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\n",
    "landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values\n",
    "\n",
    "display(df_landcover_labels.head(2))\n",
    "display(df_suggested_landcover_alignment.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        980\n",
       "1        248\n",
       "2        436\n",
       "3        520\n",
       "4        844\n",
       "        ... \n",
       "17032      3\n",
       "17033      3\n",
       "17034      3\n",
       "17035      3\n",
       "17036      3\n",
       "Name: species_id, Length: 17037, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obs.species_id.value_counts(sort=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 12533/12539 [21:49<00:00,  9.57it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17031 is out of bounds for axis 0 with size 17031",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chiro\\Documents\\GeoLifeCLEF\\torch_on_patches.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chiro/Documents/GeoLifeCLEF/torch_on_patches.ipynb#ch0000017?line=5'>6</a>\u001b[0m small_species \u001b[39m=\u001b[39m labels[counts\u001b[39m<\u001b[39maug_th]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chiro/Documents/GeoLifeCLEF/torch_on_patches.ipynb#ch0000017?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m species \u001b[39min\u001b[39;00m tqdm(small_species, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(small_species)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chiro/Documents/GeoLifeCLEF/torch_on_patches.ipynb#ch0000017?line=8'>9</a>\u001b[0m     species_count \u001b[39m=\u001b[39m counts[species]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chiro/Documents/GeoLifeCLEF/torch_on_patches.ipynb#ch0000017?line=10'>11</a>\u001b[0m     species_subset \u001b[39m=\u001b[39m df_train[df_train\u001b[39m.\u001b[39mspecies_id \u001b[39m==\u001b[39m species]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chiro/Documents/GeoLifeCLEF/torch_on_patches.ipynb#ch0000017?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(aug_th \u001b[39m-\u001b[39m species_count):\n",
      "\u001b[1;31mIndexError\u001b[0m: index 17031 is out of bounds for axis 0 with size 17031"
     ]
    }
   ],
   "source": [
    "aug_th = 50\n",
    "df_train_small = pd.DataFrame()\n",
    "\n",
    "labels, counts = np.unique(df_train.species_id, return_counts=True)\n",
    "\n",
    "small_species = labels[counts<aug_th]\n",
    "\n",
    "for species in tqdm(small_species, total=len(small_species)):\n",
    "    species_count = counts[species]\n",
    "\n",
    "    species_subset = df_train[df_train.species_id == species]\n",
    "    \n",
    "    for i in range(aug_th - species_count):\n",
    "        species_row = species_subset.sample()\n",
    "        df_train_small = pd.concat([df_train_small, species_row], ignore_index=True)\n",
    "\n",
    "df_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_train_small], ignore_index=True).reset_index()\n",
    "\n",
    "import pickle as pkl\n",
    "with open(\"./df_train_augmented.pkl\", \"wb\") as f:\n",
    "    pkl.dump(df_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2022425, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData(Dataset):\n",
    "    def __init__(self, df, data_path, load, landcover_mapping):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.load = load\n",
    "        self.landcover_mapping = landcover_mapping\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs_id = self.df.observation_id.loc[idx]\n",
    "        species = self.df.species_id.loc[idx]\n",
    "        patch = self.load(obs_id, self.data_path, landcover_mapping=self.landcover_mapping)\n",
    "        patch = [patch[0],\n",
    "                 np.reshape(patch[1], (patch[1].shape[0], patch[1].shape[1],1)),\n",
    "                  np.reshape(patch[2], (patch[2].shape[0], patch[2].shape[1],1)),\n",
    "                   np.reshape(patch[3], (patch[3].shape[0], patch[3].shape[1],1))]\n",
    "        patch = torch.FloatTensor(np.concatenate(patch, axis=2))\n",
    "        patch = torch.movedim(patch, 2, 0)\n",
    "        return patch, torch.FloatTensor([species])\n",
    "\n",
    "\n",
    "class ImageDataTest(Dataset):\n",
    "    def __init__(self, df, data_path, load, landcover_mapping):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.data_path = data_path\n",
    "        self.load = load\n",
    "        self.landcover_mapping = landcover_mapping\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obs_id = self.df.observation_id.loc[idx]\n",
    "        # species = self.df.species_id.loc[idx]\n",
    "        patch = self.load(obs_id, self.data_path, landcover_mapping=self.landcover_mapping)\n",
    "        patch = [patch[0],\n",
    "                 np.reshape(patch[1], (patch[1].shape[0], patch[1].shape[1],1)),\n",
    "                  np.reshape(patch[2], (patch[2].shape[0], patch[2].shape[1],1)),\n",
    "                   np.reshape(patch[3], (patch[3].shape[0], patch[3].shape[1],1))]\n",
    "        patch = torch.FloatTensor(np.concatenate(patch, axis=2))\n",
    "        patch = torch.movedim(patch, 2, 0)\n",
    "        return patch #, torch.FloatTensor([species])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 training:   0%|          | 0/99212 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# User params\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "batch_size_val = 8\n",
    "num_workers=0\n",
    "plt_steps=1000\n",
    "early_stop = 30\n",
    "\n",
    "# Data loader call\n",
    "# data_transf = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "train_data = ImageData(df = df_train, data_path = DATA_PATH, load = load_patch, landcover_mapping=landcover_mapping)\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = batch_size, num_workers=num_workers)\n",
    "\n",
    "val_data = ImageData(df = df_val, data_path = DATA_PATH, load = load_patch, landcover_mapping=landcover_mapping)\n",
    "val_loader = DataLoader(dataset = val_data, shuffle=False, batch_size=batch_size_val)\n",
    "# y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def training():\n",
    "\n",
    "    # Train model\n",
    "    loss_log_train = []\n",
    "    acc_log_train = []\n",
    "    loss_log_val = []\n",
    "    acc_log_val = []\n",
    "    list_top_30_train = []\n",
    "    list_top_30_val = []\n",
    "\n",
    "    stop = 0\n",
    "    total = len(df_train)//batch_size\n",
    "    total_val = len(df_val)//batch_size_val\n",
    "    m = nn.Softmax(dim=1)\n",
    "\n",
    "    for epoch in range(epochs):    \n",
    "        # Put the model in training mode (dropout layers unfreezed)\n",
    "        model.train()    \n",
    "\n",
    "        with tqdm(enumerate(train_loader), total=total) as tepoch:\n",
    "            for ii, (data, target) in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch} training\")\n",
    "\n",
    "                target = target.squeeze().long()\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)                \n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_func(m(output), target)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()  \n",
    "                scheduler.step()\n",
    "\n",
    "                if ii % plt_steps == 0:\n",
    "                    loss_log_train.append(loss.item())       \n",
    "                    \n",
    "                    # Show accuracy while training every x batches\n",
    "                    pred = torch.argmax(m(output), dim=1)\n",
    "                    acc = accuracy_score(target.cpu(), pred.cpu())\n",
    "                    acc_log_train.append(acc.item())       \n",
    "\n",
    "                    top_30_error_train = top_30_error_rate(target.cpu().detach().numpy(), m(output).cpu().detach().numpy())\n",
    "                    list_top_30_train.append(top_30_error_train)\n",
    "\n",
    "\n",
    "                    tepoch.set_postfix(loss=loss.item(), accuracy=acc, top_30=top_30_error_train)\n",
    "\n",
    "                # stop+=1\n",
    "                # if stop > early_stop:\n",
    "                #     stop = 0\n",
    "                #     break\n",
    "\n",
    "        # Save model before validation in case of memory overflow\n",
    "        export_model = \"./models/model_epoch_\"+str(epoch)+\".pth\"\n",
    "        torch.save(model, export_model)\n",
    "\n",
    "\n",
    "        # Put the model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        with tqdm(enumerate(val_loader), total=total_val) as tepoch_val:\n",
    "            for ii, (data, target) in tepoch_val:\n",
    "                tepoch_val.set_description(f\"Epoch {epoch} validation\")\n",
    "\n",
    "                target = target.squeeze().long()\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                output = model(data)                \n",
    "\n",
    "                # Compute validation loss and accuracy\n",
    "                if ii % plt_steps == 0:\n",
    "                    loss_val = loss_func(m(output), target)\n",
    "                    loss_log_val.append(loss_val.item())       \n",
    "                    \n",
    "                    # Show accuracy while training every x batches\n",
    "                    pred = torch.argmax(m(output), dim=1)\n",
    "                    acc_val = accuracy_score(target.cpu(), pred.cpu())\n",
    "                    acc_log_val.append(acc_val.item())       \n",
    "\n",
    "                    # top_30_val = predict_top_30_set(pred.cpu().detach().numpy())\n",
    "                    top_30_error_val = top_30_error_rate(target.cpu().detach().numpy(), m(output).cpu().detach().numpy())\n",
    "                    list_top_30_val.append(top_30_error_val)\n",
    "\n",
    "\n",
    "                    tepoch_val.set_postfix(val_loss=loss_val.item(), val_accuracy=acc_val, val_top_30=top_30_error_val)\n",
    "\n",
    "                # stop+=1\n",
    "                # if stop > early_stop:\n",
    "                #     stop = 0\n",
    "                #     break\n",
    "\n",
    "        print('Epoch: {} - Loss (train): {:.6f}'.format(epoch, loss.item()), \" - Accuracy (train): {:.6f}\".format(acc), \" - Top 30 error (train): {:.6f}\".format(top_30_error_train),\n",
    "                             ' - Loss (val): {:.6f}'.format(loss_val.item()), \" - Accuracy (val): {:.6f}\".format(acc_val),  \" - Top 30 error (val): {:.6f}\".format(top_30_error_val))\n",
    "\n",
    "\n",
    "        # Visualization monitoring\n",
    "        export_loss_png = \"./models/loss_epochs_\"+str(epochs)+\"_batch_size_\"+str(batch_size)+\".png\"\n",
    "        x = range(len(loss_log_train))\n",
    "        \n",
    "        fig = plt.figure(figsize=(14,8))\n",
    "        plt.subplot(131)\n",
    "        plt.plot(x, loss_log_train, label=\"train_loss\")\n",
    "        plt.plot(x, loss_log_val, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.plot(x, acc_log_train, label=\"train_acc\")\n",
    "        plt.plot(x, acc_log_val, label=\"val_acc\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.plot(x, list_top_30_train, label=\"train_top_30_error\")\n",
    "        plt.plot(x, list_top_30_val, label=\"val_top_30_error\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.suptitle(\"Training metrics\")\n",
    "        \n",
    "        fig.patch.set_facecolor('white')\n",
    "        fig.savefig(export_loss_png, transparent=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test prediction progress: : : 1139it [04:39,  4.08it/s]                        \n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_PATH = \"./submissions/\"\n",
    "\n",
    "batch_size_test = 32\n",
    "total_test = len(df_test)//batch_size_test\n",
    "m = nn.Softmax(dim=1)\n",
    "stop = 0\n",
    "early_stop = 30\n",
    "\n",
    "test_data = ImageDataTest(df = df_test, data_path = DATA_PATH, load = load_patch, landcover_mapping=landcover_mapping)\n",
    "test_loader = DataLoader(dataset = test_data, shuffle=False, batch_size=batch_size_test)\n",
    "preds = np.zeros([0,30])\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"./models/model_epoch_0_date_11_05.pth\")\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with tqdm(enumerate(test_loader), total=total_test) as tepoch_test:\n",
    "    for ii, data in tepoch_test:\n",
    "        tepoch_test.set_description(\"Test prediction progress: \")\n",
    "        \n",
    "        # Pass data to cuda and make a prediction\n",
    "        data = data.cuda()\n",
    "        output = model(data)                \n",
    "        pred = m(output)\n",
    "\n",
    "        # Convert the prediction to a numpy array\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        pred = predict_top_30_set(pred)\n",
    "        preds = np.concatenate([preds, pred], axis=0)\n",
    "\n",
    "        # #Early stopping\n",
    "        # if stop > early_stop:\n",
    "        #     stop = 0\n",
    "        #     break\n",
    "        # else:\n",
    "        #     stop+=1\n",
    "\n",
    "    preds = np.array(preds).astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission_file(SUBMISSION_PATH + \"efficient_net_1_epochs.csv\", df_obs_test.index[:preds.shape[0]], preds)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "242f126ca25018ee30c41d8e3c06b61b91865fce3a212910a69b66b4ce16b816"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
